{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "stretch-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import math\n",
    "\n",
    "FILE_MATCHES = 1\n",
    "SENTENCE_MATCHES = 1\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Check command-line arguments\n",
    "    if len(sys.argv) != 2:\n",
    "        sys.exit(\"Usage: python questions.py corpus\")\n",
    "\n",
    "    # Calculate IDF values across files\n",
    "    files = load_files(sys.argv[1])\n",
    "    file_words = {\n",
    "        filename: tokenize(files[filename])\n",
    "        for filename in files\n",
    "    }\n",
    "    file_idfs = compute_idfs(file_words)\n",
    "\n",
    "    # Prompt user for query\n",
    "    query = set(tokenize(input(\"Query: \")))\n",
    "\n",
    "    # Determine top file matches according to TF-IDF\n",
    "    filenames = top_files(query, file_words, file_idfs, n=FILE_MATCHES)\n",
    "\n",
    "    # Extract sentences from top files\n",
    "    sentences = dict()\n",
    "    for filename in filenames:\n",
    "        for passage in files[filename].split(\"\\n\"):\n",
    "            for sentence in nltk.sent_tokenize(passage):\n",
    "                tokens = tokenize(sentence)\n",
    "                if tokens:\n",
    "                    sentences[sentence] = tokens\n",
    "\n",
    "    # Compute IDF values across sentences\n",
    "    idfs = compute_idfs(sentences)\n",
    "\n",
    "    # Determine top sentence matches\n",
    "    matches = top_sentences(query, sentences, idfs, n=SENTENCE_MATCHES)\n",
    "    for match in matches:\n",
    "        print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "vocal-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(directory):\n",
    "    \"\"\"\n",
    "    Given a directory name, return a dictionary mapping the filename of each\n",
    "    `.txt` file inside that directory to the file's contents as a string.\n",
    "    \"\"\"\n",
    "    txtfiles = dict()\n",
    "    with os.scandir(directory) as files:\n",
    "        for file in files:\n",
    "            if file.path.endswith(\".txt\"):\n",
    "                with open(file.path, encoding='UTF-8') as content:\n",
    "                    txtfiles[file.name] = content.read()\n",
    "    return txtfiles\n",
    "\n",
    "def tokenize(document):\n",
    "    ignore_words = nltk.corpus.stopwords.words(\"english\")\n",
    "    words = nltk.word_tokenize(document)\n",
    "    words = [word.lower() for word in words if word.lower() not in ignore_words\n",
    "             and not all(char in list(string.punctuation) for char in word)]\n",
    "    return words\n",
    "\n",
    "def compute_idfs(documents):\n",
    "    \"\"\"\n",
    "    Given a dictionary of `documents` that maps names of documents to a list\n",
    "    of words, return a dictionary that maps words to their IDF values.\n",
    "\n",
    "    Any word that appears in at least one of the documents should be in the\n",
    "    resulting dictionary.\n",
    "    \"\"\"\n",
    "    wordset = set()\n",
    "    words_idf = dict()\n",
    "    for document in documents:\n",
    "        wordset.update(documents[document])\n",
    "    for word in wordset:\n",
    "        words_idf[word] = math.log(len(documents)/sum([word in documents[document] for document in documents]))\n",
    "    return words_idf\n",
    "\n",
    "def top_files(query, files, idfs, n):\n",
    "    \"\"\"\n",
    "    Given a `query` (a set of words), `files` (a dictionary mapping names of\n",
    "    files to a list of their words), and `idfs` (a dictionary mapping words\n",
    "    to their IDF values), return a list of the filenames of the the `n` top\n",
    "    files that match the query, ranked according to tf-idf.\n",
    "    \"\"\"\n",
    "    query_tf_idfs = dict()\n",
    "    for file in files:\n",
    "        query_tf_idfs[file] = 0\n",
    "        for word in query:\n",
    "            query_tf_idfs[file] += idfs[word] * files[file].count(word)\n",
    "    sorted_files = [file for file in sorted(query_tf_idfs, key = lambda x: query_tf_idfs[x], reverse = True)]\n",
    "    return sorted_files[:n]\n",
    "\n",
    "def top_sentences(query, sentences, idfs, n):\n",
    "    \"\"\"\n",
    "    Given a `query` (a set of words), `sentences` (a dictionary mapping\n",
    "    sentences to a list of their words), and `idfs` (a dictionary mapping words\n",
    "    to their IDF values), return a list of the `n` top sentences that match\n",
    "    the query, ranked according to idf. If there are ties, preference should\n",
    "    be given to sentences that have a higher query term density.\n",
    "    \"\"\"\n",
    "    sentence_idf = dict()\n",
    "    sentence_query_density = dict()\n",
    "    for sentence in sentences:\n",
    "        sentence_idf[sentence] = sum([idfs[word] for word in query\n",
    "                                     if word in sentences[sentence]])\n",
    "        sentence_query_density[sentence] = sum([word in query for word in sentences[sentence]])/len(sentences[sentence])\n",
    "    sorted_sentences = [sentence for sentence in\n",
    "                        sorted(sentences, key = lambda x: (sentence_idf[x], sentence_query_density[x]) , reverse = True)]\n",
    "    return sorted_sentences[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "excited-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = load_files('corpus')\n",
    "file_words = {\n",
    "    filename: tokenize(files[filename])\n",
    "    for filename in files\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "amino-limitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of supervised learning algorithms include Active learning , classification and regression.\n",
      "Python 3.0 was released on 3 December 2008.\n",
      "Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers.\n"
     ]
    }
   ],
   "source": [
    "file_idfs = compute_idfs(file_words)\n",
    "\n",
    "queries = [set(tokenize(\"What are the types of supervised learning?\")),\n",
    "           set(tokenize(\"When was Python 3.0 released?\")),\n",
    "           set(tokenize(\"How do neurons connect in a neural network?\"))]\n",
    "for query in queries:\n",
    "\n",
    "    filenames = top_files(query, file_words, file_idfs, n=FILE_MATCHES)\n",
    "\n",
    "    sentences = dict()\n",
    "    for filename in filenames:\n",
    "        for passage in files[filename].split(\"\\n\"):\n",
    "            for sentence in nltk.sent_tokenize(passage):\n",
    "                tokens = tokenize(sentence)\n",
    "                if tokens:\n",
    "                    sentences[sentence] = tokens\n",
    "\n",
    "    idfs = compute_idfs(sentences)\n",
    "\n",
    "    matches = top_sentences(query, sentences, idfs, n=SENTENCE_MATCHES)\n",
    "    for match in matches:\n",
    "        print(match)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
