{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "excited-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "qualified-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nim():\n",
    "\n",
    "    def __init__(self, initial=[1, 3, 5, 7]):\n",
    "        \"\"\"\n",
    "        Initialize game board.\n",
    "        Each game board has\n",
    "            - `piles`: a list of how many elements remain in each pile\n",
    "            - `player`: 0 or 1 to indicate which player's turn\n",
    "            - `winner`: None, 0, or 1 to indicate who the winner is\n",
    "        \"\"\"\n",
    "        self.piles = initial.copy()\n",
    "        self.player = 0\n",
    "        self.winner = None\n",
    "\n",
    "    @classmethod\n",
    "    def available_actions(cls, piles):\n",
    "        \"\"\"\n",
    "        Nim.available_actions(piles) takes a `piles` list as input\n",
    "        and returns all of the available actions `(i, j)` in that state.\n",
    "\n",
    "        Action `(i, j)` represents the action of removing `j` items\n",
    "        from pile `i` (where piles are 0-indexed).\n",
    "        \"\"\"\n",
    "        actions = set()\n",
    "        for i, pile in enumerate(piles):\n",
    "            for j in range(1, pile + 1):\n",
    "                actions.add((i, j))\n",
    "        return actions\n",
    "\n",
    "    @classmethod\n",
    "    def other_player(cls, player):\n",
    "        \"\"\"\n",
    "        Nim.other_player(player) returns the player that is not\n",
    "        `player`. Assumes `player` is either 0 or 1.\n",
    "        \"\"\"\n",
    "        return 0 if player == 1 else 1\n",
    "\n",
    "    def switch_player(self):\n",
    "        \"\"\"\n",
    "        Switch the current player to the other player.\n",
    "        \"\"\"\n",
    "        self.player = Nim.other_player(self.player)\n",
    "\n",
    "    def move(self, action):\n",
    "        \"\"\"\n",
    "        Make the move `action` for the current player.\n",
    "        `action` must be a tuple `(i, j)`.\n",
    "        \"\"\"\n",
    "        pile, count = action\n",
    "\n",
    "        # Check for errors\n",
    "        if self.winner is not None:\n",
    "            raise Exception(\"Game already won\")\n",
    "        elif pile < 0 or pile >= len(self.piles):\n",
    "            raise Exception(\"Invalid pile\")\n",
    "        elif count < 1 or count > self.piles[pile]:\n",
    "            raise Exception(\"Invalid number of objects\")\n",
    "\n",
    "        # Update pile\n",
    "        self.piles[pile] -= count\n",
    "        self.switch_player()\n",
    "\n",
    "        # Check for a winner\n",
    "        if all(pile == 0 for pile in self.piles):\n",
    "            self.winner = self.player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "rough-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NimAI():\n",
    "\n",
    "    def __init__(self, alpha=0.5, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Initialize AI with an empty Q-learning dictionary,\n",
    "        an alpha (learning) rate, and an epsilon rate.\n",
    "\n",
    "        The Q-learning dictionary maps `(state, action)`\n",
    "        pairs to a Q-value (a number).\n",
    "         - `state` is a tuple of remaining piles, e.g. (1, 1, 4, 4)\n",
    "         - `action` is a tuple `(i, j)` for an action\n",
    "        \"\"\"\n",
    "        self.q = dict()\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update(self, old_state, action, new_state, reward):\n",
    "        \"\"\"\n",
    "        Update Q-learning model, given an old state, an action taken\n",
    "        in that state, a new resulting state, and the reward received\n",
    "        from taking that action.\n",
    "        \"\"\"\n",
    "        old = self.get_q_value(old_state, action)\n",
    "        best_future = self.best_future_reward(new_state)\n",
    "        self.update_q_value(old_state, action, old, reward, best_future)\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        \"\"\"\n",
    "        Return the Q-value for the state `state` and the action `action`.\n",
    "        If no Q-value exists yet in `self.q`, return 0.\n",
    "        \"\"\"\n",
    "        if (tuple(state), action) in self.q:\n",
    "            return self.q[(tuple(state), action)]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def update_q_value(self, state, action, old_q, reward, future_rewards):\n",
    "        \"\"\"\n",
    "        Update the Q-value for the state `state` and the action `action`\n",
    "        given the previous Q-value `old_q`, a current reward `reward`,\n",
    "        and an estiamte of future rewards `future_rewards`.\n",
    "\n",
    "        Use the formula:\n",
    "\n",
    "        Q(s, a) <- old value estimate\n",
    "                   + alpha * (new value estimate - old value estimate)\n",
    "\n",
    "        where `old value estimate` is the previous Q-value,\n",
    "        `alpha` is the learning rate, and `new value estimate`\n",
    "        is the sum of the current reward and estimated future rewards.\n",
    "        \"\"\"\n",
    "        self.q[(tuple(state), action)] = old_q + self.alpha * (reward + future_rewards - old_q)\n",
    "\n",
    "    def best_future_reward(self, state):\n",
    "        \"\"\"\n",
    "        Given a state `state`, consider all possible `(state, action)`\n",
    "        pairs available in that state and return the maximum of all\n",
    "        of their Q-values.\n",
    "\n",
    "        Use 0 as the Q-value if a `(state, action)` pair has no\n",
    "        Q-value in `self.q`. If there are no available actions in\n",
    "        `state`, return 0.\n",
    "        \"\"\"\n",
    "        best_reward = 0\n",
    "        for action in Nim.available_actions(state):\n",
    "            if self.get_q_value(state, action) > best_reward:\n",
    "                best_reward = self.get_q_value(state, action)\n",
    "        return best_reward\n",
    "\n",
    "    def choose_action(self, state, epsilon=True):\n",
    "        \"\"\"\n",
    "        Given a state `state`, return an action `(i, j)` to take.\n",
    "\n",
    "        If `epsilon` is `False`, then return the best action\n",
    "        available in the state (the one with the highest Q-value,\n",
    "        using 0 for pairs that have no Q-values).\n",
    "\n",
    "        If `epsilon` is `True`, then with probability\n",
    "        `self.epsilon` choose a random available action,\n",
    "        otherwise choose the best action available.\n",
    "\n",
    "        If multiple actions have the same Q-value, any of those\n",
    "        options is an acceptable return value.\n",
    "        \"\"\"\n",
    "        actions = Nim.available_actions(state)\n",
    "        chosen_action = random.choice(list(actions))\n",
    "        q_value = 0\n",
    "        for action in actions:\n",
    "            if self.get_q_value(state, action) > q_value:\n",
    "                q_value = self.get_q_value(state, action)\n",
    "                chosen_action = action\n",
    "        if epsilon and random.random() <= self.epsilon:\n",
    "            chosen_action = random.choice(list(actions))\n",
    "        return chosen_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "finnish-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n):\n",
    "    \"\"\"\n",
    "    Train an AI by playing `n` games against itself.\n",
    "    \"\"\"\n",
    "\n",
    "    player = NimAI()\n",
    "\n",
    "    # Play n games\n",
    "    for i in range(n):\n",
    "        print(f\"Playing training game {i + 1}\")\n",
    "        game = Nim()\n",
    "\n",
    "        # Keep track of last move made by either player\n",
    "        last = {\n",
    "            0: {\"state\": None, \"action\": None},\n",
    "            1: {\"state\": None, \"action\": None}\n",
    "        }\n",
    "\n",
    "        # Game loop\n",
    "        while True:\n",
    "\n",
    "            # Keep track of current state and action\n",
    "            state = game.piles.copy()\n",
    "            action = player.choose_action(game.piles)\n",
    "\n",
    "            # Keep track of last state and action\n",
    "            last[game.player][\"state\"] = state\n",
    "            last[game.player][\"action\"] = action\n",
    "\n",
    "            # Make move\n",
    "            game.move(action)\n",
    "            new_state = game.piles.copy()\n",
    "\n",
    "            # When game is over, update Q values with rewards\n",
    "            if game.winner is not None:\n",
    "                player.update(state, action, new_state, -1)\n",
    "                player.update(\n",
    "                    last[game.player][\"state\"],\n",
    "                    last[game.player][\"action\"],\n",
    "                    new_state,\n",
    "                    1\n",
    "                )\n",
    "                break\n",
    "\n",
    "            # If game is continuing, no rewards yet\n",
    "            elif last[game.player][\"state\"] is not None:\n",
    "                player.update(\n",
    "                    last[game.player][\"state\"],\n",
    "                    last[game.player][\"action\"],\n",
    "                    new_state,\n",
    "                    0\n",
    "                )\n",
    "\n",
    "    print(\"Done training\")\n",
    "\n",
    "    # Return the trained AI\n",
    "    return player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "proved-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(ai, human_player=None):\n",
    "    \"\"\"\n",
    "    Play human game against the AI.\n",
    "    `human_player` can be set to 0 or 1 to specify whether\n",
    "    human player moves first or second.\n",
    "    \"\"\"\n",
    "\n",
    "    # If no player order set, choose human's order randomly\n",
    "    if human_player is None:\n",
    "        human_player = random.randint(0, 1)\n",
    "\n",
    "    # Create new game\n",
    "    game = Nim()\n",
    "\n",
    "    # Game loop\n",
    "    while True:\n",
    "\n",
    "        # Print contents of piles\n",
    "        print()\n",
    "        print(\"Piles:\")\n",
    "        for i, pile in enumerate(game.piles):\n",
    "            print(f\"Pile {i}: {pile}\")\n",
    "        print()\n",
    "\n",
    "        # Compute available actions\n",
    "        available_actions = Nim.available_actions(game.piles)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Let human make a move\n",
    "        if game.player == human_player:\n",
    "            print(\"Your Turn\")\n",
    "            while True:\n",
    "                pile = int(input(\"Choose Pile: \"))\n",
    "                count = int(input(\"Choose Count: \"))\n",
    "                if (pile, count) in available_actions:\n",
    "                    break\n",
    "                print(\"Invalid move, try again.\")\n",
    "\n",
    "        # Have AI make a move\n",
    "        else:\n",
    "            print(\"AI's Turn\")\n",
    "            pile, count = ai.choose_action(game.piles, epsilon=False)\n",
    "            print(f\"AI chose to take {count} from pile {pile}.\")\n",
    "\n",
    "        # Make move\n",
    "        game.move((pile, count))\n",
    "\n",
    "        # Check for winner\n",
    "        if game.winner is not None:\n",
    "            print()\n",
    "            print(\"GAME OVER\")\n",
    "            winner = \"Human\" if game.winner == human_player else \"AI\"\n",
    "            print(f\"Winner is {winner}\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "crazy-bridge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing training game 1\n",
      "Playing training game 2\n",
      "Playing training game 3\n",
      "Playing training game 4\n",
      "Playing training game 5\n",
      "Playing training game 6\n",
      "Playing training game 7\n",
      "Playing training game 8\n",
      "Playing training game 9\n",
      "Playing training game 10\n",
      "Playing training game 11\n",
      "Playing training game 12\n",
      "Playing training game 13\n",
      "Playing training game 14\n",
      "Playing training game 15\n",
      "Playing training game 16\n",
      "Playing training game 17\n",
      "Playing training game 18\n",
      "Playing training game 19\n",
      "Playing training game 20\n",
      "Playing training game 21\n",
      "Playing training game 22\n",
      "Playing training game 23\n",
      "Playing training game 24\n",
      "Playing training game 25\n",
      "Playing training game 26\n",
      "Playing training game 27\n",
      "Playing training game 28\n",
      "Playing training game 29\n",
      "Playing training game 30\n",
      "Playing training game 31\n",
      "Playing training game 32\n",
      "Playing training game 33\n",
      "Playing training game 34\n",
      "Playing training game 35\n",
      "Playing training game 36\n",
      "Playing training game 37\n",
      "Playing training game 38\n",
      "Playing training game 39\n",
      "Playing training game 40\n",
      "Playing training game 41\n",
      "Playing training game 42\n",
      "Playing training game 43\n",
      "Playing training game 44\n",
      "Playing training game 45\n",
      "Playing training game 46\n",
      "Playing training game 47\n",
      "Playing training game 48\n",
      "Playing training game 49\n",
      "Playing training game 50\n",
      "Playing training game 51\n",
      "Playing training game 52\n",
      "Playing training game 53\n",
      "Playing training game 54\n",
      "Playing training game 55\n",
      "Playing training game 56\n",
      "Playing training game 57\n",
      "Playing training game 58\n",
      "Playing training game 59\n",
      "Playing training game 60\n",
      "Playing training game 61\n",
      "Playing training game 62\n",
      "Playing training game 63\n",
      "Playing training game 64\n",
      "Playing training game 65\n",
      "Playing training game 66\n",
      "Playing training game 67\n",
      "Playing training game 68\n",
      "Playing training game 69\n",
      "Playing training game 70\n",
      "Playing training game 71\n",
      "Playing training game 72\n",
      "Playing training game 73\n",
      "Playing training game 74\n",
      "Playing training game 75\n",
      "Playing training game 76\n",
      "Playing training game 77\n",
      "Playing training game 78\n",
      "Playing training game 79\n",
      "Playing training game 80\n",
      "Playing training game 81\n",
      "Playing training game 82\n",
      "Playing training game 83\n",
      "Playing training game 84\n",
      "Playing training game 85\n",
      "Playing training game 86\n",
      "Playing training game 87\n",
      "Playing training game 88\n",
      "Playing training game 89\n",
      "Playing training game 90\n",
      "Playing training game 91\n",
      "Playing training game 92\n",
      "Playing training game 93\n",
      "Playing training game 94\n",
      "Playing training game 95\n",
      "Playing training game 96\n",
      "Playing training game 97\n",
      "Playing training game 98\n",
      "Playing training game 99\n",
      "Playing training game 100\n",
      "Done training\n",
      "\n",
      "Piles:\n",
      "Pile 0: 1\n",
      "Pile 1: 3\n",
      "Pile 2: 5\n",
      "Pile 3: 7\n",
      "\n",
      "Your Turn\n",
      "Choose Pile: 1\n",
      "Choose Count: 2\n",
      "\n",
      "Piles:\n",
      "Pile 0: 1\n",
      "Pile 1: 1\n",
      "Pile 2: 5\n",
      "Pile 3: 7\n",
      "\n",
      "AI's Turn\n",
      "AI chose to take 1 from pile 3.\n",
      "\n",
      "Piles:\n",
      "Pile 0: 1\n",
      "Pile 1: 1\n",
      "Pile 2: 5\n",
      "Pile 3: 6\n",
      "\n",
      "Your Turn\n",
      "Choose Pile: 2\n",
      "Choose Count: 4\n",
      "\n",
      "Piles:\n",
      "Pile 0: 1\n",
      "Pile 1: 1\n",
      "Pile 2: 1\n",
      "Pile 3: 6\n",
      "\n",
      "AI's Turn\n",
      "AI chose to take 1 from pile 1.\n",
      "\n",
      "Piles:\n",
      "Pile 0: 1\n",
      "Pile 1: 0\n",
      "Pile 2: 1\n",
      "Pile 3: 6\n",
      "\n",
      "Your Turn\n",
      "Choose Pile: 6\n",
      "Choose Count: 5\n",
      "Invalid move, try again.\n",
      "Choose Pile: 3\n",
      "Choose Count: 5\n",
      "\n",
      "Piles:\n",
      "Pile 0: 1\n",
      "Pile 1: 0\n",
      "Pile 2: 1\n",
      "Pile 3: 1\n",
      "\n",
      "AI's Turn\n",
      "AI chose to take 1 from pile 3.\n",
      "\n",
      "Piles:\n",
      "Pile 0: 1\n",
      "Pile 1: 0\n",
      "Pile 2: 1\n",
      "Pile 3: 0\n",
      "\n",
      "Your Turn\n",
      "Choose Pile: 2\n",
      "Choose Count: 1\n",
      "\n",
      "Piles:\n",
      "Pile 0: 1\n",
      "Pile 1: 0\n",
      "Pile 2: 0\n",
      "Pile 3: 0\n",
      "\n",
      "AI's Turn\n",
      "AI chose to take 1 from pile 0.\n",
      "\n",
      "GAME OVER\n",
      "Winner is Human\n"
     ]
    }
   ],
   "source": [
    "ai = train(100)\n",
    "play(ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aggregate-ozone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "front-guitar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(list(set([1,2,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "rural-adult",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "tester = list(set([1,2,2]))\n",
    "for test in tester:\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-mills",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
